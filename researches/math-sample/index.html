<!doctype html><html lang=en itemscope itemtype=http://schema.org/Article><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Coline Metta-Versmessen"><meta name=description content="PhD student at Dauphine, PSL university, Climate Economics Chair, EDF R&amp;D"><meta name=keywords content='academic-folio,academic-website,al-folio,hugo,hugo-theme'><meta property="og:url" content="https://artmorse.github.io/coline-academic-folio/researches/math-sample/"><meta property="og:site_name" content="Academic Folio"><meta property="og:title" content="Math stuff"><meta property="og:description" content="Having some math here is good."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="researches"><meta property="article:published_time" content="2022-03-18T21:10:00+07:00"><meta property="article:modified_time" content="2022-03-18T21:10:00+07:00"><meta itemprop=name content="Math stuff"><meta itemprop=description content="Having some math here is good."><meta itemprop=datePublished content="2022-03-18T21:10:00+07:00"><meta itemprop=dateModified content="2022-03-18T21:10:00+07:00"><meta itemprop=wordCount content="957"><title>Math stuff - Academic Folio
</title><link rel="shortcut icon" href=/coline-academic-folio/favicon.svg type=image/svg+xml><link rel=canonical href=https://artmorse.github.io/coline-academic-folio/researches/math-sample/><link rel=stylesheet href=/tailwind.css><style>:root{--global-text-color-primary:#171717;--global-text-color-shade:#525252;--global-bg-color:#f5f5f5;--global-theme-color:#2563eb;--global-header-color:#f5f5f5;--global-footer-color:#f5f5f5;--global-divider-color:#525252}:root #theme-toggle .icon-sun{display:none}:root #theme-toggle .icon-moon{display:inline}html[class=dark]{--global-text-color-primary:#f5f5f5;--global-text-color-shade:#a3a3a3;--global-bg-color:#171717;--global-theme-color:#38bdf8;--global-header-color:#171717;--global-footer-color:#171717;--global-divider-color:#a3a3a3}html[class=dark] #theme-toggle .icon-sun{display:inline}html[class=dark] #theme-toggle .icon-moon{display:none}main{max-width:1024px;margin:0 auto}header nav{max-width:1024px}html{scroll-behavior:smooth}a{color:var(--global-text-color-primary);text-decoration:underline}a:hover{color:var(--global-theme-color)}#TableOfContents ul li{margin-top:.5rem}#TableOfContents ul li a{text-decoration:none}.icon{display:inline;width:1em;height:1em;fill:currentColor;vertical-align:-.14em}.katex-display{overflow:auto hidden}</style><style>.e-mail:before{content:attr(data-domain)"\0040" attr(data-user);unicode-bidi:bidi-override;direction:rtl}</style><script>function toggleTheme(){document.documentElement.classList.toggle("dark"),localStorage.theme=localStorage.theme==="dark"?"light":"dark",typeof setGiscusTheme=="function"&&setGiscusTheme(localStorage.theme)}!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches||localStorage.theme==="dark"?(document.documentElement.classList.add("dark"),localStorage.theme="dark"):localStorage.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0},{left:"\\[",right:"\\]",display:!0}],throwOnError:!0})'></script></head><body class="mx-auto text-primary bg-background"><header class="fixed inset-x-0 top-0 text-primary bg-header z-10 border-b border-divider"><nav class="mx-auto p-4 text-base font-medium flex md:flex-row flex-col"><ul class="flex flex-row"><li><a class="whitespace-nowrap font-bold no-underline" href=https://artmorse.github.io/coline-academic-folio/>Academic Folio</a></li><li class="ml-auto pl-4 text-[1.25em] overflow-x-auto overflow-y-hidden"><ul class="social flex flex-row flex-nowrap space-x-4 justify-center"><li><a href=mailto:c.mettaversmessen@gmail.com title=email target=_blank rel="nofollow noopener me"><svg class="envelope icon" viewBox="0 0 512 512"><path d="M48 64C21.5 64 0 85.5.0 112c0 15.1 7.1 29.3 19.2 38.4L236.8 313.6c11.4 8.5 27 8.5 38.4.0L492.8 150.4c12.1-9.1 19.2-23.3 19.2-38.4.0-26.5-21.5-48-48-48H48zM0 176V384c0 35.3 28.7 64 64 64H448c35.3.0 64-28.7 64-64V176L294.4 339.2c-22.8 17.1-54 17.1-76.8.0L0 176z"/></svg></a></li><li><a href=https://www.linkedin.com/username title=LinkedIn target=_blank rel="nofollow noopener me"><svg class="linkedin icon" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></a></li></ul></li><li class="md:hidden ml-4 w-4"></li></ul><ul class="flex flex-row md:ml-auto md:mt-0 mt-4"><li class="md:w-fit overflow-auto"><ul class="flex flex-row flex-nowrap space-x-4"><li><a href=/coline-academic-folio/researches/ class="whitespace-nowrap no-underline">Research</a></li><li><a href=/coline-academic-folio/teaching/ class="whitespace-nowrap no-underline">Teaching</a></li><li><a href=/coline-academic-folio/events/ class="whitespace-nowrap no-underline">Scientific Events</a></li><li><a href=/coline-academic-folio/cv/ class="whitespace-nowrap no-underline">Curriculum Vitae</a></li></ul></li><li class="absolute md:static top-4 right-4 md:ml-4 md:mt-0"><button id=theme-toggle class=hover:text-theme onclick=toggleTheme()>
<span class=icon-sun title="light theme"><svg class="sun icon" viewBox="0 0 512 512"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM352 256c0 53-43 96-96 96s-96-43-96-96 43-96 96-96 96 43 96 96zm32 0c0-70.7-57.3-128-128-128s-128 57.3-128 128 57.3 128 128 128 128-57.3 128-128z"/></svg>
</span><span class=icon-moon title="dark theme"><svg class="moon icon" viewBox="0 0 384 512"><path d="M223.5 32C1e2 32 0 132.3.0 256S1e2 480 223.5 480c60.6.0 115.5-24.2 155.8-63.4 5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6-96.9.0-175.5-78.8-175.5-176 0-65.8 36-123.1 89.3-153.3 6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"/></svg></span></button></li></ul></nav></header><main class="mt-24 md:mt-14 mb-12 md:mb-8 px-4 py-8 space-y-8"><header class=space-y-2><h1 class="text-4xl font-medium">Math stuff</h1><h2 class=font-medium>Having some math here is good.</h2></header><article class=markdown><h2 id=introduction>Introduction</h2><p>Many articles like <a href=https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7 target=_blank rel="external nofollow noopener">this</a>
in the <a href=https://towardsdatascience.com/ target=_blank rel="external nofollow noopener">towardsdatascience</a>
tell us how to use Bayesian Linear Regression and why. I am not going to repeat that. I want to derive the formulas from scratch entirely.</p><p>I also understand that comprehending this stuff is slightly challenging if you don&rsquo;t have a strong mathematics background.</p><blockquote><p>Note: All of my notation is from Bishop’s book.</p></blockquote><p>The main difference between Bayesian and Non-Bayesian regression is that Bayesian regression assumes that weights are Random variables.</p><p>First, we should define the distribution of <strong>w</strong> and <strong>e</strong> and also our target function. We assume that we have a Gaussian prior over <strong>w,</strong> which is our weight vector, So</p><p>$$
w \sim N(m_0,s_0)
$$</p><p>which means that we have m0 as our mean vector, and s0 is our covariance matrix. Now, if we write our target function, we would have:</p><p>$$
\begin{aligned}
y &=f(x, w)+\epsilon \
&=w^{T} \phi(x)+\epsilon
\end{aligned}
$$</p><p>In which <strong>φ</strong> is any transformation on x. If you are not familiar with transformations, assume that φ(x) = x, and you&rsquo;re good to go.</p><p>Just one thing is missing, what is the distribution of error?</p><p>$$
\epsilon \sim N\left(0, \sigma^{2}\right)=N\left(0, \beta^{-1}\right)
$$</p><blockquote><p>Note: we define β as the noise precision parameter, which is the reciprocal noise variance.</p></blockquote><p>The Likelihood function would be:</p><p>$$
p(y \mid X, w, \beta)=\prod_{i=1}^{N} p\left(y_{n} \mid x_{n}, w, b\right)=\prod_{i=1}^{N} N\left(y_{n} \mid w^{T} \phi\left(x_{n}\right), \beta^{-1}\right)
$$</p><p>Next, we can compute the posterior distribution, which is proportional to the product of likelihood and prior. Note that we already know that because the prior is Gaussian, the posterior would be gaussian too! But As I said, I want to compute that to make sure everything is right entirely.</p><p>$$
\text { posterior } \propto \text { likelihood } \times \text { prior }
$$</p><h2 id=deriving-prior-and-posterior-distributions>Deriving prior and posterior distributions</h2><p>The posterior is computed by the usual procedure of <strong>completing the square</strong>, so I ignore all constants and focus on the power of exponentials. Again, notice that we want to compute the mean and variance of the posterior.</p><p>From likelihood perspective, we have:</p><p>$$
\log \text { likelihood }=\frac{-\beta}{2} \sum_{i=1}^{N}\left(y_{n}-w^{T} \phi(x)\right)^{2}
$$</p><p>and from prior perspective, we have:</p><p>$$
\text { log prior }=\frac{-1}{2}\left(w-m_{0}\right)^{T} S_{0}^{-1}\left(w-m_{0}\right)
$$</p><p>You already know that product in logarithm would result in sum. So we have:</p><p>$$
\begin{aligned}
\log \text { Posterior } &=\frac{-\beta}{2} \sum_{n=1}^{N}\left(y_{n}-w^{T} \phi(x)\right)^{2}+\frac{-1}{2}\left(w-m_{0}\right)^{T} S_{0}^{-1}\left(w-m_{0}\right) \
&\left.=\frac{-\beta}{2} \sum_{n=1}^{N}\left(y_{n}^{2}-2 y_{n} w^{T} \phi(x)+w^{T} \phi\left(x_{n}\right) \phi\left(x_{n}\right)^{T} w\right)\right)+\frac{-1}{2}\left(w-m_{0}\right)^{T} S_{0}{ }^{1}\left(w-m_{0}\right) \
&=\frac{-1}{2} w^{T}\left[\sum_{n=1}^{N} \beta \phi\left(x_{n}\right) \phi\left(x_{n}\right)^{T}+S_{0}^{-1}\right] w+\frac{-1}{2}\left[-2 m_{0}^{T} s_{0}-\frac{1}{2} \sum_{n=1}^{N} 2 \beta y_{n} \phi\left(x_{n}\right)^{T}\right] w+const
\end{aligned}
$$</p><p>Then from comparing the powers with standard Gaussian, we have:</p><p>$$
S_{N}^{-1}=S_{0}^{-1}+\beta \phi^{T} \phi
$$</p><p>and by comparing the second expression we get:</p><p>$$
\begin{array}{c}
-2 m_{N}^{T} S_{N}^{-1}=-2 m_{0}^{T} S_{0}^{-1}-2 \beta y^{T} \phi \
m_{N}=S_{N}^{T}\left(S_{0}^{T}\right)^{-1} m_{0}+S_{N}^{T} \beta y^{T} \phi
\end{array}
$$</p><p>So we computed the mean and variance of the posterior distribution, but It is not the end. We should use predictive posterior to derive the final result.</p><h2 id=deriving-predictive-posterior-distribution>Deriving predictive posterior distribution</h2><p>Let’s get back to our big picture. We computed the posterior, which is the distribution of weights, but what is the distribution of our prediction (y*) when we have new data (x*)?</p><p>Let’s say our new data is (<em><em>x</em>,y</em>**), and we want to derive the distribution of y* given x*. What should we do? The answer lies in <strong>predictive distribution.</strong></p><p>Wait. We can have one slight improvement here. Because we can always normalize the input data to have a mean of 0, we can assume that our weight prior is also Normal with a mean of 0. So we can take that:</p><p>$$
p(w \mid \alpha)=N\left(w \mid 0, \alpha^{-1} I\right)
$$</p><p>So the posterior mean and variance would be:</p><p>$$
\begin{array}{r}
M_{N}=\beta S_{N} \phi^{T} y \
S_{N}=\alpha I+\beta \phi^{T} \phi
\end{array}
$$</p><p>We can not go further until we know how to get the predictive posterior. So the predictive posterior distribution is defined as:</p><p>$$
p\left(y^{*} \mid x^{*}, x, y, \alpha, \beta\right)=\int p\left(y^{*} \mid x^{*}, w, \beta\right) p\left(w \mid x^{*}, x, y, \alpha, \beta\right) d w
$$</p><p>The first time I saw this formula, I wanted to run away and live in the desert. Bear with me, All we’re doing is integrating out <strong>w</strong> since we don’t know what it is. Sometimes, there is a more straightforward way. Fortunately, a proven formula can help us skip the integration part and get to the result.</p><blockquote><p>Note: If you want to prove it yourself, you can look at Marginal and conditional Gaussian from the bishop’s book.</p></blockquote><p><strong>Theorem:</strong> Given a marginal distribution for <strong>x</strong> and a conditional distribution for <strong>y</strong> in the form:</p><p>$$
\begin{aligned}
p(x) &=N\left(x \mid \mu, \Lambda^{-1}\right) \
p(y \mid x) &=N\left(y \mid A x+b, L^{-1}\right)
\end{aligned}
$$</p><p>the marginal distribution of y is given by:</p><p>$$
p(y)=N\left(y \mid A \mu+b, L^{-1}+A \Lambda^{-1} A^{T}\right)
$$</p><h2 id=mixing-all-stuff-together>Mixing all stuff together</h2><p>So, we have all the ingredients. Let’s get them in one bowl and mix them together.</p><p>$$
\begin{aligned}
p\left(w \mid x^{*}, x, y, \alpha, \beta\right) &=N\left(w \mid M_{N}, S_{N}\right) \
p\left(y^{*} \mid w, \beta\right) &=N\left(f(x, w), \beta^{-1}\right)
\end{aligned}
$$</p><p>Look at these two for one moment. The p(x) is equivalent to the first equation and p(y|x) is like the second equation. If we plug it in, we get the following equations (Check yourself):</p><p>$$
\begin{aligned}
p\left(y^{*} \mid y, x, x^{*}, \alpha, \beta\right) &=N\left(y^{*} \mid M_{N}^{T} \phi(x), \sigma_{N}^{2}(x)\right) \
\sigma_{N}^{2}(x) &=\beta^{-1} \phi^{T}(x) S_{N} \phi(x)
\end{aligned}
$$</p><p>And finally, if we plug all these values into equation p(y), we get the following:</p><p>$$
\begin{aligned}
p\left(y^{*} \mid y, x, x^{*}, \alpha, \beta\right) &=N\left(y^{*} \mid M_{N}^{T} \phi(x), \sigma_{N}^{2}(x)\right) \
\sigma_{N}^{2}(x) &=\beta^{-1} \phi^{T}(x) S_{N} \phi(x)
\end{aligned}
$$</p><p>pfff, we got the mean and variance of y*. Now we are certainly finished. I hope this helped you!</p><p>Thank you for reading.</p></article></main><footer class="fixed inset-x-0 bottom-0 p-2 bg-footer text-primary text-center text-xs border-t border-divider">© Copyright 2023 The Academic Folio Contributors
&bull;</footer></body></html>